{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Analyzing, Filtering, and Cleaning Aviation Database for the Safest Plane\n",
    "                                         Jupyter Notebook coded by Allison Ward, Rick Lataille, and Anthony Mansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Goal: \n",
    "______________________________________________________________________________________________________________________\n",
    "### The main thing we want coming out of the analysis is idenifying planes with the lowest risk in the U.S.\n",
    "## Source of Data: \n",
    "______________________________________________________________________________________________________________________\n",
    "### Our initial data set was pulled from the National Transportation Safety Boardâ€™s (NTSB) Aviation Accident data set from 1962 to 2023. The data contains information regarding civil aviation accidents and selected incidents in the United States and international waters. The data set we will be using is filled with about 90,000 rows of accidents involving all types of aircrafts. \n",
    "______________________________________________________________________________________________________________________\n",
    "## Limitations:\n",
    "### The dataset used in this analysis was provided by The Flatiron School. It only shows planes in accidents; we do not know how many flights took place overall, so we cannot normalize our data. Additionally, it does not differentiate between hardware failures and pilot error. Therefore, the scope of our analysis is limited.\n",
    "\n",
    "### And of course, to start off the project in Python fashion, imported pandas to manipulate our new dataframe buddy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{90348}\n"
     ]
    }
   ],
   "source": [
    "#Import the modules we need\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# And this is the big data that we will be using\n",
    "df = pd.read_csv('data/Aviation_Data.csv', low_memory=False)\n",
    "\n",
    "print({len(df)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, we will be dropping columns we see no need for, the information won't help us with our goal. These columns are dropped because the information it provides has no use in finding the results our stakeholder is looking for, or even WE can't use it to find other helpful data beyond that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90348 items.\n"
     ]
    }
   ],
   "source": [
    "# Drop the columns we know that we don't need\n",
    "dropped_columns = ['Schedule', 'Report.Status', 'Publication.Date']\n",
    "df.drop(columns = dropped_columns, inplace=True)\n",
    "print(f\"{len(df)} items.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now looking at the rest of the columns, we filter some columns for the information we need. We leave the original \"df\" alone and instead make another variable to hold our filtered information. The ways we filtered were: \n",
    "* Filtering for rows with data from the last 10 years + turned into date-time, and also created days of the week using those, * * Filter data for aircrafts to airplanes only since that's the data we will be using\n",
    "* Exclude the rows for planes that are amateur built since they... kind of screw the results we need over\n",
    "* Filtered even more plane uses to continue providing relevant data adjusted to our stakeholder\n",
    "* Filtering for the United States only. Filtering for the U.S. only is mainly because the rows that aren't in here aren't filled and can't tell of anything, and it even the same case for the U.S. territories that aren't between the Altantic and Pacific Oceans. Dropped from about 90,000 items to 7,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15829 items.\n",
      "13262 items.\n",
      "11726 items.\n",
      "9497 items.\n",
      "7320 items.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-aa051abb3ac1>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['Day_Of_Week'] = df['Event.Date'].dt.day_name()\n"
     ]
    }
   ],
   "source": [
    "# Convert date column to datetime, then filter event dates to include 2013 and later\n",
    "df['Event.Date'] = pd.to_datetime(df['Event.Date'])\n",
    "df_filtered = df.loc[df['Event.Date'] >= '2013-01-01']\n",
    "print(f\"{len(df_filtered)} items.\")\n",
    "\n",
    "# Creating a new column with Day of Week\n",
    "df_filtered['Day_Of_Week'] = df['Event.Date'].dt.day_name()\n",
    "\n",
    "# Filter aircraft categories for Airplanes only\n",
    "df_filtered = df_filtered.loc[df_filtered['Aircraft.Category'] == 'Airplane']\n",
    "print(f\"{len(df_filtered)} items.\")\n",
    "\n",
    "# Exclude Amateur-built planes\n",
    "df_filtered = df_filtered.loc[df_filtered['Amateur.Built'] != 'Yes']\n",
    "print(f\"{len(df_filtered)} items.\")\n",
    "\n",
    "# Exclude certain identified purposes as irrelevant to our stakeholder\n",
    "allowed_purposes = ['Personal', np.nan, 'Business', 'Executive/corporate', \\\n",
    "                    'Positioning', 'Other Work Use', 'Ferry', 'Unknown', 'Public Aircraft - Federal', \\\n",
    "                   'Public Aircraft - State', 'Public Aircraft - Local', 'Public Aircraft', 'PUBS']\n",
    "df_filtered = df_filtered.loc[df_filtered['Purpose.of.flight'].isin(allowed_purposes)]\n",
    "print(f\"{len(df_filtered)} items.\")\n",
    "\n",
    "# Include only events that happened in the United States or US Territories\n",
    "allowed_countries = ['United States']\n",
    "df_filtered = df_filtered.loc[df_filtered['Country'].isin(allowed_countries)]\n",
    "print(f\"{len(df_filtered)} items.\")\n",
    "\n",
    "# Drop even more columns that are no longer useful\n",
    "obsolete_columns = ['Event.Id', 'Country', 'Aircraft.Category', 'Registration.Number', 'Broad.phase.of.flight']\n",
    "df_filtered.drop(columns = obsolete_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alriight, now that we've done as much filtering we believe we need, we move on to cleaning the remaining columns.  And if you threw the CSV into Tableau, you'd notice that-- even though we filtered for the United States specifically-- some of the points in the map are NOT in the United States, or even U.S. territories. So the cell block below was to try and limit the lies the data was telling us, or just lazy humans being human or something. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7307 items.\n",
      "7302 items.\n"
     ]
    }
   ],
   "source": [
    "# Filter for foreign locations not noted as foreign using the 'OF' state code in Location\n",
    "df_filtered['State_Code'] = df_filtered['Location'].str.slice(-2)\n",
    "df_filtered = df_filtered.loc[df_filtered['State_Code'] != 'OF']\n",
    "print(f\"{len(df_filtered)} items.\")\n",
    "\n",
    "# Drop rows that are missing latitude coordinates (also captures missing Longitude)\n",
    "df_filtered.dropna(subset=['Latitude'], inplace=True)\n",
    "print(f\"{len(df_filtered)} items.\")\n",
    "\n",
    "#Converting latitude and longitude from Degrees, Minutes, and Seconds to Decimal Degrees\n",
    "\n",
    "df_filtered.dropna(subset=['Latitude', 'Longitude'], inplace=True)\n",
    "\n",
    "def convert_latitude(x):\n",
    "    degrees = float(x[:2])\n",
    "    minutes = float(x[2:4])\n",
    "    seconds = float(x[4:6])\n",
    "    return degrees + minutes/60 + seconds/3600\n",
    "\n",
    "df_filtered[\"new_lats\"] = df_filtered['Latitude'].map(convert_latitude)\n",
    "\n",
    "def convert_longitude(x):\n",
    "    degrees = float(x[:3])\n",
    "    minutes = float(x[3:5])\n",
    "    seconds = float(x[5:7])\n",
    "    return -(degrees + minutes/60 + seconds/3600)\n",
    "\n",
    "df_filtered[\"new_longs\"] = df_filtered['Longitude'].map(convert_longitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As you may know, Python is case sensitive, and the data does NOT reflect that. And because of that, things that should be grouped together will be grouped seperatively based on one just starting with \"A\" instead of \"a.\" So it's time to fix that. :) The original_makes variable wasn't useful in the long run, it was just as a way to see how many duplicates were left and things like that. Lamba function was used to try and clean the names. I know its a LOT of the same line repeating pretty much, but knowing the natural language processing or any other methods is currently beyond our levels, so unfortunately we have to do what we had to do. Lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record original makes for later comparison\n",
    "Original_makes = len(df_filtered['Make'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These map functions will clean the 'Make' column to focus on the makes in our analysis\n",
    "df_filtered['Make'] = df_filtered['Make'].map(lambda x: \"Airbus\" if x.lower().strip()[:6]==\"airbus\" else x)\n",
    "df_filtered['Make'] = df_filtered['Make'].map(lambda x: \"Airbus\" if x.lower().strip()[:5]==\"fouga\" else x)\n",
    "df_filtered['Make'] = df_filtered['Make'].map(lambda x: \"Boeing\" if x.lower().strip()[:6]==\"boeing\" else x)\n",
    "df_filtered['Make'] = df_filtered['Make'].map(lambda x: \"Boeing\" if x.lower().strip()[:9]==\"mcdonnell\" else x)\n",
    "df_filtered['Make'] = df_filtered['Make'].map(lambda x: \"Boeing\" if x.lower().strip()[:7]==\"douglas\" else x)\n",
    "df_filtered['Make'] = df_filtered['Make'].map(lambda x: \"Boeing\" if x.lower().strip()[:8]==\"rockwell\" else x)\n",
    "df_filtered['Make'] = df_filtered['Make'].map(lambda x: \"Bombardier\" if x.lower().strip()[:10]==\"bombardier\" else x)\n",
    "df_filtered['Make'] = df_filtered['Make'].map(lambda x: \"Bombardier\" if x.lower().strip()[:5]==\"gates\" else x)\n",
    "df_filtered['Make'] = df_filtered['Make'].map(lambda x: \"Bombardier\" if x.lower().strip()[:7]==\"learjet\" else x)\n",
    "df_filtered['Make'] = df_filtered['Make'].map(lambda x: \"Bombardier\" if x.lower().strip()[:8]==\"canadair\" else x)\n",
    "df_filtered['Make'] = df_filtered['Make'].map(lambda x: \"Dassault\" if x.lower().strip()[:8]==\"dassault\" else x)\n",
    "df_filtered['Make'] = df_filtered['Make'].map(lambda x: \"Embraer\" if x.lower().strip()[:7]==\"embraer\" else x)\n",
    "df_filtered['Make'] = df_filtered['Make'].map(lambda x: \"Gulfstream\" if x.lower().strip()[:10]==\"gulfstream\" else x)\n",
    "df_filtered['Make'] = df_filtered['Make'].map(lambda x: \"Gulfstream\" if x.lower().strip()[:3]==\"iai\" else x)\n",
    "df_filtered['Make'] = df_filtered['Make'].map(lambda x: \"Honda\" if x.lower().strip()[:5]==\"honda\" else x)\n",
    "df_filtered['Make'] = df_filtered['Make'].map(lambda x: \"Textron\" if x.lower().strip()[:6]==\"cessna\" else x)\n",
    "df_filtered['Make'] = df_filtered['Make'].map(lambda x: \"Textron\" if x.lower().strip()[:4]==\"rath\" else x)\n",
    "df_filtered['Make'] = df_filtered['Make'].map(lambda x: \"Textron\" if x.lower().strip()[:4]==\"rayt\" else x)\n",
    "df_filtered['Make'] = df_filtered['Make'].map(lambda x: \"Textron\" if x.lower().strip()[:7]==\"textron\" else x)\n",
    "df_filtered['Make'] = df_filtered['Make'].map(lambda x: \"Textron\" if x.lower().strip()[:5]==\"beech\" else x)\n",
    "df_filtered['Make'] = df_filtered['Make'].map(lambda x: \"Textron\" if x.lower().strip()[:6]==\"hawker\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original 670 makes have been reduced to 595 makes.\n"
     ]
    }
   ],
   "source": [
    "# Show the amount of consolidation in makes\n",
    "print(f\"The original {Original_makes} makes have been reduced to {len(df_filtered['Make'].unique())} makes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting rid of whack \"NaN\", \"None\", or \"Uknown\" by appropriately filling in values for data usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change \"NaN\" to 'None' or 'Unknown', as appropriate\n",
    "df_filtered['Injury.Severity'].fillna('None', inplace=True)\n",
    "df_filtered['Aircraft.damage'].fillna('Unknown', inplace=True)\n",
    "df_filtered['Purpose.of.flight'].fillna('Unknown', inplace=True)\n",
    "df_filtered['Engine.Type'].fillna('Unknown', inplace=True)\n",
    "df_filtered['FAR.Description'].fillna('Unknown', inplace=True)\n",
    "df_filtered['Number.of.Engines'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# This will convert all 'unknown' type entries to 'Unknown' in the Air.carrier field\n",
    "df_filtered['Air.carrier'].fillna('Unknown', inplace=True)\n",
    "df_filtered['Air.carrier'] = df_filtered['Air.carrier'].astype(str).map(\n",
    "    lambda x: \"Unknown\" if x.lower().strip()[:3]==\"unk\" else x)\n",
    "\n",
    "# This will convert all 'unknown' type entries to 'Unknown' in the Weather.Condition field\n",
    "df_filtered['Weather.Condition'].fillna('Unknown', inplace=True)\n",
    "df_filtered['Weather.Condition'] = df_filtered['Weather.Condition'].astype(str).map(\n",
    "    lambda x: \"Unknown\" if x.lower().strip()[:3]==\"unk\" else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All of these lines have completely different functions, and some is more filtering than cleaning, or neither, but had to make a little more moves to get even MORE of the data comparisons we wanted to include."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all Makes into Title case, for readability\n",
    "df_filtered['Make'] = df_filtered['Make'].map(lambda x: x.title())\n",
    "\n",
    "# Use dt functions to extract year and month and create new columns\n",
    "df_filtered['Year'] = df['Event.Date'].dt.year\n",
    "df_filtered['Month'] = df['Event.Date'].dt.month\n",
    "\n",
    "# Create a new column to simplify the large jet analysis\n",
    "separate_large_jets = [\"Airbus\", \"Boeing\", \"Embraer\"]\n",
    "df_filtered['Large_Jets'] = df_filtered['Make'].map(lambda x: \"Other\" if x not in separate_large_jets else x)\n",
    "\n",
    "# Create a new column to simplify the large jet analysis\n",
    "separate_small_jets = [\"Bombardier\", \"Dassault\", \"Gulfstream\", \"Honda\", \"Textron\"]\n",
    "df_filtered['Small_Jets'] = df_filtered['Make'].map(lambda x: \"Other\" if x not in separate_small_jets else x)\n",
    "\n",
    "# Create a new column summing fatal and serious injuries\n",
    "df_filtered['Major_Injuries'] = df_filtered['Total.Fatal.Injuries'] + df_filtered['Total.Serious.Injuries']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With all that filtered data, saved it to a new csv file for an even easier time making visualizations of the data since all the info we need won't be surrounded by the other  random information. We also did this instead of overwriting the original \"just in case\", y'know? Just in case we wanted to undo something, we could simply overwrite the csv and throw it back into Tableau real fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to a new CSV file/overwrites CSV file\n",
    "df_filtered.to_csv('Filtered_Aviation_Data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
